---
publishDate: 2024-08-12T00:00:00Z
title: FastAPI ile Llama Modeli Kullan覺m覺
excerpt: Bu makalede, FastAPI'de Llama modelini nas覺l kullanaca覺m覺z覺 a癟覺klayaca覺m. Llama2 modelini kullanarak basit bir API oluturaca覺z.
image: /images/blog/programming/fast-llama.webp
author: Abdulhamit Celik
category: Programming
tags: []
---

## Llama Nedir?

Llama (Large Language Model Meta AI'nin k覺saltmas覺 ve 繹nceki ekliyle LLaMA olarak stilize edilmitir), Meta AI taraf覺ndan ubat 2023'te balayarak yay覺nlanan oto-regresif b羹y羹k dil modelleri (LLM) ailesidir. En son s羹r羹m羹 Temmuz 2024'te yay覺nlanan Llama 3.1'dir.

## FastAPI'de Llama Kullan覺m覺

FastAPI kullanarak her yerde kullan覺labilen bir llama servisi oluturaca覺z.

ncelikle ba覺ml覺l覺klar覺 y羹kleyeceiz:

```bash
pip install fastapi uvicorn llama-cpp-python
```

Daha sonra modeli huggingface reposundan 癟ekmek i癟in bir yard覺mc覺 y繹ntem yazaca覺z:

```python
from llama_cpp import Llama


def init_model():
    print("Loading model...")
    llm = Llama.from_pretrained(
        repo_id="Qwen/Qwen2-0.5B-Instruct-GGUF", filename="*q8_0.gguf", verbose=False
    )
    print("Model loaded.")
    return llm
```

Bu yard覺mc覺 y繹ntemle modeli y羹kleyebilir ve balang覺癟ta FastAPI uygulamam覺zda kullanabiliriz, bu y羹zden uygulama balamadan 繹nce modeli y羹klemek i癟in bir `@asynccontextmanager` oluturaca覺z:

```python
# other imports
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    app.state.llm = init_model()
    yield

app = FastAPI(lifespan=lifespan)
```

Bu ekilde uygulamay覺 oluturduumuzda model y羹klenecek ve router'lar覺m覺zda kullanabileceiz. Bir dependency oluturarak modeli router'lar覺m覺zda kullanabiliriz:

```python
def get_llm():
    return app.state.llm

@app.post("/question")
def get_answer(llm: Llama = Depends(get_llm)):
    pass
```

G繹r羹ld羹羹 gibi, llama modelimizi bir dependency olarak router'覺m覺zda kullan覺yoruz.

imdi bu modeli kullanarak bir soru sormak i癟in bir endpoint olutural覺m:

```python
from pydantic import BaseModel

class Question(BaseModel):
    q: str

@app.post("/question")
def get_answer(data: Question, llm: Llama = Depends(get_llm)):
    pass
```

ve istekte body i癟erisinden gelicek olan data deikenini kullanarak soruyu llama modeline soraca覺z.

```python
@app.post("/question")
def get_answer(data: Question, llm: Llama = Depends(get_llm)):
        answer = llm(
        f"Q: {data.q} A:",  # Prompt
        max_tokens=32,  # Generate up to 32 tokens, set to None to generate up to the end of the context window
        stop=[
            "Q:",
            "\n",
        ],  # Stop generating just before the model would generate a new question
    )
    return {"answer":answer["choices"][0]["text"]}
```

Art覺k uygulamay覺 癟al覺t覺rabilirsiniz:

```bash
uvicorn main:app --reload
```

ve test edelim:

```bash
curl -X 'POST' \
  'http://localhost:8000/question' \
    -H 'accept: application/json' \
    -H 'Content-Type: application/json' \
    -d '{
    "q": "What is capital of France?"
}'
```

Hepsi bu kadar! Llama modelini kullanarak sorular覺 yan覺tlayan basit bir FastAPI uygulamas覺 oluturdunuz. Herhangi bir sorunuz varsa, sormaktan 癟ekinmeyin.

rnek Kod: https://github.com/hmtcelik/fast-llama

Mutlu kodlamalar! 
